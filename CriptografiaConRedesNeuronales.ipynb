{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "CriptografiaConRedesNeuronales.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms1R67EfxOCk"
      },
      "source": [
        "# Criptografia con el uso de redes Neuronales\n",
        "\n",
        "\n",
        "En este cuaderno, veremos cómo construir una red neuronal recurrente y entrenarla para descifrar cadenas cifradas con un determinado cifrado.\n",
        "\n",
        "Este ejercicio lo familiarizará con las técnicas de preprocesamiento y construcción de modelos que le serán útiles cuando comience a construir modelos más avanzados para traducción automática, resumen de texto y más.\n",
        "\n",
        "## DataSet\n",
        "El dataset que tengo consta de 10,000 frases encriptadas y la versión en texto plano de cada frase encriptada.\n",
        "\n",
        "Comencemos cargando el conjunto de datos para familiarizarnos con él. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KysFJ-0fxP7Q",
        "outputId": "96b4969f-86d5-47f6-bc25-e931f5b3a7b6"
      },
      "source": [
        "\n",
        "!wget -c https://raw.githubusercontent.com/franciscoventurablancas/CIC_IPN/master/Patrones/textoPlano.txt\n",
        "!wget -c https://raw.githubusercontent.com/franciscoventurablancas/CIC_IPN/master/Patrones/cifrado.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-11 20:43:34--  https://raw.githubusercontent.com/franciscoventurablancas/CIC_IPN/master/Patrones/textoPlano.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2021-10-11 20:43:34--  https://raw.githubusercontent.com/franciscoventurablancas/CIC_IPN/master/Patrones/cifrado.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqUyM9FE5OoR"
      },
      "source": [
        "Cargamos los archivos que vamos utilizar 1 archivo con el texto plano y 1 archivo con el teto cifrado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTcimflcx6Al"
      },
      "source": [
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFK4Y41xOCv"
      },
      "source": [
        "import os\n",
        "\n",
        "codes = load_data('cifrado.txt')\n",
        "plaintext = load_data('textoPlano.txt')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mBSNVFbxOC6"
      },
      "source": [
        "Ahora, los \"códigos\" y el \"texto sin formato\" son matrices y cada elemento es una frase. Las primeras cinco frases codificadas son: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMdMeJPwxOC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f244ffc5-9174-4299-8037-41f36c050dc8"
      },
      "source": [
        "codes[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['YMJ QNRJ NX MJW QJFXY QNPJI KWZNY , GZY YMJ GFSFSF NX RD QJFXY QNPJI .',\n",
              " 'MJ XFB F TQI DJQQTB YWZHP .',\n",
              " 'NSINF NX WFNSD IZWNSL OZSJ , FSI NY NX XTRJYNRJX BFWR NS STAJRGJW .',\n",
              " 'YMFY HFY BFX RD RTXY QTAJI FSNRFQ .',\n",
              " 'MJ INXQNPJX LWFUJKWZNY , QNRJX , FSI QJRTSX .']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avnOW9VuxOC-"
      },
      "source": [
        "Y sus versiones de texto sin ser encriptadas son: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NJvilOkxODA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6065e677-c173-4308-e873-5d9e40f4a073"
      },
      "source": [
        "plaintext[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['THE LIME IS HER LEAST LIKED FRUIT , BUT THE BANANA IS MY LEAST LIKED .',\n",
              " 'HE SAW A OLD YELLOW TRUCK .',\n",
              " 'INDIA IS RAINY DURING JUNE , AND IT IS SOMETIMES WARM IN NOVEMBER .',\n",
              " 'THAT CAT WAS MY MOST LOVED ANIMAL .',\n",
              " 'HE DISLIKES GRAPEFRUIT , LIMES , AND LEMONS .']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClSquXr_xODC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ff8d0d-04d3-4eda-d9c0-0cecaae98ed1"
      },
      "source": [
        "len(codes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns2pFQfTxODX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f833f5-572a-4552-9d3a-55fa0c2eb2f8"
      },
      "source": [
        "len(plaintext)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWI3UqSxODZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222e687f-832b-4d94-80ba-58c472ccb218"
      },
      "source": [
        "max([len(sentence) for sentence in codes])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDq4pjNb7rm4"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Ni7X9ixODc"
      },
      "source": [
        "## Descripción general del modelo: RNN a nivel de  carácter\n",
        "El modelo que usaremos aquí es un RNN(Red neuronal recurrente) a nivel de carácteres ya que el cifrado parece funcionar en el nivel de carácter. En un escenario de traducción automática, una RNN a nivel de palabra es la opción más común.\n",
        "\n",
        "Un RNN a nivel de carácter tomará como entrada un entero que se refiere a un carácter específico y dará salida a otro entero. Para que nuestro modelo funcione, necesitaremos preprocesar nuestro conjunto de datos en los siguientes pasos:\n",
        " 1. Aislar cada carácter como un elemento de la matriz (en lugar de una frase completa o una palabra como elemento de la matriz)\n",
        " 2. Tokenizar los caracteres para que podamos convertirlos de letras a números enteros y viceversa\n",
        " 3. Rellenar las cadenas para que todas las entradas y salidas quepan en forma de matriz\n",
        " \n",
        "Para visualizar este procesamiento, supongamos que nuestras secuencias de origen (\"códigos\" en este caso) o secuencias de destino (\"texto sin formato\" en este caso) se ven así (una lista de cadenas):\n",
        "\n",
        "<img src = \"https://github.com/LaurentVeyssier/Cracking_code_using_RNN_at-character_level/blob/main/list_1.png?raw=1\" />\n",
        "\n",
        "Dado que este modelo funcionará en el nivel de carácter, necesitaremos separar cada cadena en una lista de caracteres (hecho implícitamente por el tokenizador en este cuaderno):\n",
        "\n",
        "<img src = \"https://github.com/LaurentVeyssier/Cracking_code_using_RNN_at-character_level/blob/main/list_2.png?raw=1\" />\n",
        "\n",
        "Luego, el proceso de tokenización convertirá cada carácter en un número entero. Tenga en cuenta que cuando está trabajando en un RNN a nivel de palabra (como en la mayoría de los ejemplos de traducción automática), el tokenizador asignará un número entero a cada palabra en lugar de a cada letra, y cada celda representaría una palabra en lugar de un carácter.\n",
        "\n",
        "<img src = \"https://github.com/LaurentVeyssier/Cracking_code_using_RNN_at-character_level/blob/main/list_3.png?raw=1\" />\n",
        "\n",
        "La mayoría de las plataformas de aprendizaje automático esperan que la entrada sea una matriz en lugar de una lista de listas. Para convertir la entrada en una matriz, necesitamos encontrar el miembro más largo de la lista y rellenar todas las secuencias más cortas con 0. Suponiendo que 'uno y dos' es la secuencia más larga en este ejemplo, la matriz termina luciendo así:\n",
        "\n",
        "<img src = \"https://github.com/LaurentVeyssier/Cracking_code_using_RNN_at-character_level/blob/main/padded_list.png?raw=1\" />\n",
        " \n",
        "## Preprocesamiento \n",
        "Para que una red neuronal prediga sobre datos de texto, primero debe convertirse en datos que pueda comprender. Los datos de texto como \"perro\" son una secuencia de codificaciones de caracteres ASCII. Dado que una red neuronal es una serie de operaciones de multiplicación y suma, los datos de entrada deben ser números.\n",
        "\n",
        "Podemos convertir cada carácter en un número o cada palabra en un número. Estos se denominan identificadores de caracteres y palabras, respectivamente. Los identificadores de caracteres se utilizan para modelos de nivel de caracteres que generan predicciones de texto para cada caracter. Un modelo de nivel de palabra utiliza identificadores de palabras que generan predicciones de texto para cada palabra. Los modelos de nivel de palabra tienden a aprender mejor.\n",
        "\n",
        "Convierta cada oración en una secuencia de identificadores de palabras usando la función [`Tokenizer`] (https://keras.io/preprocessing/text/#tokenizer) de Keras. Ya que estamos trabajando en el nivel del caracteres, asegúrese de establecer el indicador `char_level` en el valor apropiado. Luego, coloque el tokenizador en x. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDEX_ynGxODi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047b8747-d2ce-4981-c27f-26570bec1232"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \n",
        "    tf.keras.preprocessing.text.Tokenizer(num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=\" \",\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    document_count=0,\n",
        "    **kwargs\n",
        "    )\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    x_tk = Tokenizer(char_level=True)\n",
        "    x_tk.fit_on_texts(x)                 # because input is text, not sequence (list of integer tokens)\n",
        "\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'e': 2, 'o': 3, 't': 4, 'i': 5, 's': 6, 'h': 7, 'r': 8, 'y': 9, 'u': 10, 'c': 11, 'n': 12, 'a': 13, 'p': 14, '.': 15, 'q': 16, 'k': 17, 'b': 18, 'w': 19, 'f': 20, 'x': 21, 'j': 22, 'm': 23, 'v': 24, 'l': 25, 'z': 26, 'd': 27, 'g': 28, ',': 29}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [4, 7, 2, 1, 16, 10, 5, 11, 17, 1, 18, 8, 3, 19, 12, 1, 20, 3, 21, 1, 22, 10, 23, 14, 6, 1, 3, 24, 2, 8, 1, 4, 7, 2, 1, 25, 13, 26, 9, 1, 27, 3, 28, 1, 15]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [18, 9, 1, 22, 3, 24, 2, 1, 29, 1, 23, 9, 1, 16, 10, 5, 11, 17, 1, 6, 4, 10, 27, 9, 1, 3, 20, 1, 25, 2, 21, 5, 11, 3, 28, 8, 13, 14, 7, 9, 1, 19, 3, 12, 1, 13, 1, 14, 8, 5, 26, 2, 1, 15]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [4, 7, 5, 6, 1, 5, 6, 1, 13, 1, 6, 7, 3, 8, 4, 1, 6, 2, 12, 4, 2, 12, 11, 2, 1, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i5AMoUDxODj"
      },
      "source": [
        "### Padding (IMPLEMENTACIÓN)\n",
        "Al agrupar la secuencia de ID de palabras, cada secuencia debe tener la misma longitud. Dado que las oraciones tienen una longitud dinámica, podemos agregar relleno al final de las secuencias para que tengan la misma longitud.\n",
        "\n",
        "Asegúrese de que todas las secuencias de cifrado tengan la misma longitud y que todas las secuencias de texto sin formato tengan la misma longitud agregando relleno al ** final ** de cada secuencia usando [`pad_sequences`] de Keras (https://keras.io/preprocessing/ función secuencia / # pad_sequences). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2tQjklnxODk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15c827d-d6c0-43f3-ba85-48d61c6418f8"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \n",
        "    tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre',\n",
        "    value=0.0)\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    # Find the length of the longest string in the dataset.\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    # Then, pass it to pad_sentences as the maxlen parameter\n",
        "    \n",
        "    return pad_sequences(x, maxlen=length, padding=\"post\", truncating=\"post\",)\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [ 4  7  2  1 16 10  5 11 17  1 18  8  3 19 12  1 20  3 21  1 22 10 23 14\n",
            "  6  1  3 24  2  8  1  4  7  2  1 25 13 26  9  1 27  3 28  1 15]\n",
            "  Output: [ 4  7  2  1 16 10  5 11 17  1 18  8  3 19 12  1 20  3 21  1 22 10 23 14\n",
            "  6  1  3 24  2  8  1  4  7  2  1 25 13 26  9  1 27  3 28  1 15  0  0  0\n",
            "  0  0  0  0  0  0]\n",
            "Sequence 2 in x\n",
            "  Input:  [18  9  1 22  3 24  2  1 29  1 23  9  1 16 10  5 11 17  1  6  4 10 27  9\n",
            "  1  3 20  1 25  2 21  5 11  3 28  8 13 14  7  9  1 19  3 12  1 13  1 14\n",
            "  8  5 26  2  1 15]\n",
            "  Output: [18  9  1 22  3 24  2  1 29  1 23  9  1 16 10  5 11 17  1  6  4 10 27  9\n",
            "  1  3 20  1 25  2 21  5 11  3 28  8 13 14  7  9  1 19  3 12  1 13  1 14\n",
            "  8  5 26  2  1 15]\n",
            "Sequence 3 in x\n",
            "  Input:  [ 4  7  5  6  1  5  6  1 13  1  6  7  3  8  4  1  6  2 12  4  2 12 11  2\n",
            "  1 15]\n",
            "  Output: [ 4  7  5  6  1  5  6  1 13  1  6  7  3  8  4  1  6  2 12  4  2 12 11  2\n",
            "  1 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5hk9hbhxODn"
      },
      "source": [
        "### Pipeline de preproceso\n",
        "Su enfoque para este proyecto es construir una arquitectura de red neuronal, por lo que no le pediremos que cree una canalización de preproceso. En su lugar, le proporcionamos la implementación de la función `preprocess`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6MEFsH6xODq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5877c0db-a2af-4887-b930-3430dbed5b79"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_code_sentences, preproc_plaintext_sentences, code_tokenizer, plaintext_tokenizer =\\\n",
        "    preprocess(codes, plaintext)\n",
        "\n",
        "print('Data Preprocessed')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBYJwkQ7xODr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ae2b77-1334-4c3f-84dd-dcb26de972bb"
      },
      "source": [
        "preproc_code_sentences[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5, 14,  3,  1, 10,  2, 13,  3,  1,  2,  4,  1, 14,  3,  6,  1, 10,\n",
              "        3,  8,  4,  5,  1, 10,  2, 25,  3, 11,  1, 20,  6,  9,  2,  5,  1,\n",
              "       18,  1, 17,  9,  5,  1,  5, 14,  3,  1, 17,  8,  7,  8,  7,  8,  1,\n",
              "        2,  4,  1, 13, 15,  1, 10,  3,  8,  4,  5,  1, 10,  2, 25,  3, 11,\n",
              "        1, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVW6wdgDxODu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfb2d35-c8cd-4b64-dd59-6b1c8fff30fe"
      },
      "source": [
        "len(code_tokenizer.word_index)+1  # +1 for 0?"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPqf5Zo0xODv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b78d41d-9ef8-4fe1-ace7-90d0df5eeb81"
      },
      "source": [
        "len(plaintext_tokenizer.word_index)+1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soV6yQZxODx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83525415-e1c0-410d-c534-440a87be8eee"
      },
      "source": [
        "plaintext_tokenizer.word_index"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 1,\n",
              " \"'\": 31,\n",
              " ',': 18,\n",
              " '.': 19,\n",
              " '?': 30,\n",
              " 'a': 8,\n",
              " 'b': 17,\n",
              " 'c': 22,\n",
              " 'd': 11,\n",
              " 'e': 3,\n",
              " 'f': 20,\n",
              " 'g': 16,\n",
              " 'h': 14,\n",
              " 'i': 2,\n",
              " 'j': 26,\n",
              " 'k': 25,\n",
              " 'l': 10,\n",
              " 'm': 13,\n",
              " 'n': 7,\n",
              " 'o': 12,\n",
              " 'p': 21,\n",
              " 'q': 28,\n",
              " 'r': 6,\n",
              " 's': 4,\n",
              " 't': 5,\n",
              " 'u': 9,\n",
              " 'v': 23,\n",
              " 'w': 24,\n",
              " 'x': 27,\n",
              " 'y': 15,\n",
              " 'z': 29}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubz35Ae8xODz"
      },
      "source": [
        "De forma predeterminada, la salida de una capa RNN contiene un solo vector por muestra. Este vector es la salida de la celda RNN correspondiente al último paso de tiempo, que contiene información sobre la secuencia de entrada completa. La forma de esta salida es (batch_size, units) donde las unidades corresponden al argumento de las unidades pasado al constructor de la capa.\n",
        "\n",
        "Una capa RNN también puede devolver la secuencia completa de salidas para cada muestra (un vector por paso de tiempo por muestra), si establece return_sequences = True. La forma de esta salida es (tamaño de lote, pasos de tiempo, unidades).\n",
        "\n",
        "Además, una capa RNN puede devolver sus estados internos finales. Los estados devueltos se pueden utilizar para reanudar la ejecución de RNN más tarde o para inicializar otro RNN. Esta configuración se usa comúnmente en el modelo de secuencia a secuencia del codificador-decodificador, donde el estado final del codificador se usa como el estado inicial del decodificador.\n",
        "\n",
        "Para configurar una capa RNN para que devuelva su estado interno, establezca el parámetro return_state en True al crear la capa. Tenga en cuenta que LSTM tiene 2 tensores de estado, pero GRU solo tiene uno.\n",
        "\n",
        "Para configurar el estado inicial de la capa, simplemente llame a la capa con el argumento de palabra clave adicional initial_state. Tenga en cuenta que la forma del estado debe coincidir con el tamaño de la unidad de la capa\n",
        "\n",
        "retorna el estado:\n",
        "\n",
        "- output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(x)\n",
        "- encoder_state = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4sRC2fJxOD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c40574-35c9-43a5-ed96-048ea551b9e1"
      },
      "source": [
        "preproc_code_sentences.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-30OntlxOD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9232b6a-bd0f-4d7c-c601-932e6e7994ef"
      },
      "source": [
        "preproc_plaintext_sentences.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 101, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLF-nT2cxOD7"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OgCjA9VxOD8"
      },
      "source": [
        "from keras.layers import GRU, Input, Dense, TimeDistributed, RNN, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import Adam # - Works\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow\n",
        "\n",
        "\n",
        "def simple_model(input_shape, output_sequence_length, code_vocab_size, plaintext_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param code_vocab_size: Number of unique code characters in the dataset\n",
        "    :param plaintext_vocab_size: Number of unique plaintext characters in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Build the model\n",
        "    \n",
        "    x = Input(shape=input_shape[1:])   # shape(none,101,1) ie \n",
        "    \n",
        "    seq = LSTM(units= 64, return_sequences = True, activation=\"tanh\", name='Layer1')(x)  # output must be batchsize x timesteps x units\n",
        "    \n",
        "    output = TimeDistributed(Dense(units = plaintext_vocab_size, activation='softmax', name='Layer2'))(seq)\n",
        "    \n",
        "    model = Model(inputs = x, outputs = output)\n",
        "    \n",
        "    model.compile(optimizer='adam', loss= sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_code_sentences, preproc_plaintext_sentences.shape[1]) # pad code sequences with maxlen 54: shape=10001x54\n",
        "tmp_x = tmp_x.reshape((-1, preproc_plaintext_sentences.shape[-2], 1))     # reshape padded code seq in 10001 x 54 x 1\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5qcMQrwxOD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe967bc-88b5-412a-93aa-c4793be3a434"
      },
      "source": [
        "tmp_x.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 101, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl6-B37SxOED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7063e4-a50c-4966-ff3e-fc7a6b0368a0"
      },
      "source": [
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_plaintext_sentences.shape[1],\n",
        "    len(code_tokenizer.word_index)+1,\n",
        "    len(plaintext_tokenizer.word_index)+1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 101, 1)]          0         \n",
            "_________________________________________________________________\n",
            "Layer1 (LSTM)                (None, 101, 64)           16896     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 101, 32)           2080      \n",
            "=================================================================\n",
            "Total params: 18,976\n",
            "Trainable params: 18,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpCanb70xOEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46b18c6-4b27-4212-c493-b706aadf140f"
      },
      "source": [
        "simple_rnn_model.get_layer(name=\"Layer1\").output.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 101, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PrWZu-DxOEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1882ca-3113-4ea2-ef14-59687b917271"
      },
      "source": [
        "simple_rnn_model.fit(tmp_x, preproc_plaintext_sentences, batch_size=32, epochs=4, validation_split=0.2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "250/250 [==============================] - 14s 29ms/step - loss: 1.5790 - accuracy: 0.5458 - val_loss: 1.0690 - val_accuracy: 0.6770\n",
            "Epoch 2/4\n",
            "250/250 [==============================] - 7s 27ms/step - loss: 0.8815 - accuracy: 0.7421 - val_loss: 0.7319 - val_accuracy: 0.7969\n",
            "Epoch 3/4\n",
            "250/250 [==============================] - 7s 26ms/step - loss: 0.6180 - accuracy: 0.8380 - val_loss: 0.5228 - val_accuracy: 0.8679\n",
            "Epoch 4/4\n",
            "250/250 [==============================] - 7s 26ms/step - loss: 0.4498 - accuracy: 0.8901 - val_loss: 0.3875 - val_accuracy: 0.9090\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f344052e9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vo2ng1bxOEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4e3a36-fc7b-408e-a489-4f6e30443c19"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')\n",
        "\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], plaintext_tokenizer))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`logits_to_text` function loaded.\n",
            "h h e   l i m e   i s   m e r   l e a s t   l i f e d   . r u i t   ,   b u t   t h e   m a n a n a   i s   m o   l e a s t   l i f e d   . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lhVpa1pxOEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6a5c4e8-3aa1-4a36-9afa-084d5c8fe977"
      },
      "source": [
        "plaintext[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'THE LIME IS HER LEAST LIKED FRUIT , BUT THE BANANA IS MY LEAST LIKED .'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqi0LeWTxOEK"
      },
      "source": [
        "#GRU (Gated Recurrent Unit) \n",
        "tiene como objetivo resolver el problema del \n",
        "gradiente de desaparición que viene con una red neuronal recurrente estándar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvXZIBUJxOEK"
      },
      "source": [
        "def simple_model1(input_shape, output_sequence_length, code_vocab_size, plaintext_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param code_vocab_size: Number of unique code characters in the dataset\n",
        "    :param plaintext_vocab_size: Number of unique plaintext characters in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Build the model\n",
        "    \n",
        "    x = Input(shape=input_shape[1:])   # shape(none,54,1) ie \n",
        "    \n",
        "    seq = GRU(units= 64, return_sequences = True, activation=\"tanh\", name='Layer1')(x)  # output must be batchsize x timesteps x units\n",
        "    \n",
        "    output = TimeDistributed(Dense(units = plaintext_vocab_size, activation='softmax', name='Layer2'))(seq)\n",
        "    \n",
        "    model = Model(inputs = x, outputs = output)\n",
        "    \n",
        "    model.compile(optimizer='adam', loss= sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aq1Nr83xOEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc64145-d2d1-43f2-b1d1-ce2a43c35928"
      },
      "source": [
        "# Train the neural network\n",
        "simple_rnn_model1 = simple_model1(\n",
        "    tmp_x.shape,\n",
        "    preproc_plaintext_sentences.shape[1],\n",
        "    len(code_tokenizer.word_index)+1,\n",
        "    len(plaintext_tokenizer.word_index)+1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 101, 1)]          0         \n",
            "_________________________________________________________________\n",
            "Layer1 (GRU)                 (None, 101, 64)           12864     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 101, 32)           2080      \n",
            "=================================================================\n",
            "Total params: 14,944\n",
            "Trainable params: 14,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLpcI2OrxOEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4061928a-777f-4efa-ddb5-dcbc92755357"
      },
      "source": [
        "simple_rnn_model1.fit(tmp_x, preproc_plaintext_sentences, batch_size=128, epochs=16, validation_split=0.2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "63/63 [==============================] - 4s 34ms/step - loss: 2.4952 - accuracy: 0.4382 - val_loss: 1.6317 - val_accuracy: 0.4725\n",
            "Epoch 2/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 1.4500 - accuracy: 0.5365 - val_loss: 1.3084 - val_accuracy: 0.6051\n",
            "Epoch 3/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 1.1915 - accuracy: 0.6505 - val_loss: 1.0918 - val_accuracy: 0.6783\n",
            "Epoch 4/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 1.0005 - accuracy: 0.7129 - val_loss: 0.9230 - val_accuracy: 0.7366\n",
            "Epoch 5/16\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.8502 - accuracy: 0.7531 - val_loss: 0.7886 - val_accuracy: 0.7731\n",
            "Epoch 6/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.7289 - accuracy: 0.7985 - val_loss: 0.6793 - val_accuracy: 0.8133\n",
            "Epoch 7/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.6294 - accuracy: 0.8330 - val_loss: 0.5882 - val_accuracy: 0.8502\n",
            "Epoch 8/16\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.5457 - accuracy: 0.8662 - val_loss: 0.5103 - val_accuracy: 0.8795\n",
            "Epoch 9/16\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.4739 - accuracy: 0.8899 - val_loss: 0.4439 - val_accuracy: 0.8972\n",
            "Epoch 10/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.4125 - accuracy: 0.9054 - val_loss: 0.3866 - val_accuracy: 0.9123\n",
            "Epoch 11/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.3607 - accuracy: 0.9183 - val_loss: 0.3396 - val_accuracy: 0.9218\n",
            "Epoch 12/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.3182 - accuracy: 0.9303 - val_loss: 0.3005 - val_accuracy: 0.9355\n",
            "Epoch 13/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.2824 - accuracy: 0.9405 - val_loss: 0.2673 - val_accuracy: 0.9435\n",
            "Epoch 14/16\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.2516 - accuracy: 0.9476 - val_loss: 0.2387 - val_accuracy: 0.9487\n",
            "Epoch 15/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.2255 - accuracy: 0.9535 - val_loss: 0.2145 - val_accuracy: 0.9560\n",
            "Epoch 16/16\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.2034 - accuracy: 0.9584 - val_loss: 0.1940 - val_accuracy: 0.9610\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33cdbcab50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBrLxEVxOEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b749988-a048-4c14-b3c9-7dd4469f7273"
      },
      "source": [
        "print(logits_to_text(simple_rnn_model1.predict(tmp_x[:1])[0], plaintext_tokenizer))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t h e   l i m e   i s   h e r   l e a s t   l i k e d   f r u i t   ,   b u t   t h e   b a n a n a   i s   m o   l e a s t   l i k e d   . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUXn4qoxOEP"
      },
      "source": [
        "# Una red neuronal recurrente (RNN)\n",
        "Es una clase de redes neuronales artificiales donde las conexiones entre nodos forman un gráfico dirigido a lo largo de una secuencia temporal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC-L5UvwxOEP"
      },
      "source": [
        "from keras.layers import SimpleRNN \n",
        "def simple_model2(input_shape, output_sequence_length, code_vocab_size, plaintext_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param code_vocab_size: Number of unique code characters in the dataset\n",
        "    :param plaintext_vocab_size: Number of unique plaintext characters in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Build the model\n",
        "    \n",
        "    x = Input(shape=input_shape[1:])   # shape(none,54,1) ie \n",
        "    \n",
        "    seq = SimpleRNN(units= 64, return_sequences = True, activation=\"tanh\", name='Layer1')(x)  # output must be batchsize x timesteps x units\n",
        "    \n",
        "    output = TimeDistributed(Dense(units = plaintext_vocab_size, activation='softmax', name='Layer2'))(seq)\n",
        "    \n",
        "    model = Model(inputs = x, outputs = output)\n",
        "    \n",
        "    model.compile(optimizer='adam', loss= sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce87Asn2xOEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08c2617-7a02-470d-e435-34efbc92ca13"
      },
      "source": [
        "# Train the neural network\n",
        "simple_rnn_model2 = simple_model2(\n",
        "    tmp_x.shape,\n",
        "    preproc_plaintext_sentences.shape[1],\n",
        "    len(code_tokenizer.word_index)+1,\n",
        "    len(plaintext_tokenizer.word_index)+1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 101, 1)]          0         \n",
            "_________________________________________________________________\n",
            "Layer1 (SimpleRNN)           (None, 101, 64)           4224      \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 101, 32)           2080      \n",
            "=================================================================\n",
            "Total params: 6,304\n",
            "Trainable params: 6,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBHovlrvxOES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30776087-44c6-4e51-e588-3d5bdbcc990a"
      },
      "source": [
        "simple_rnn_model2.fit(tmp_x, preproc_plaintext_sentences, batch_size=128, epochs=16, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "63/63 [==============================] - 9s 121ms/step - loss: 2.1609 - accuracy: 0.4174 - val_loss: 1.5379 - val_accuracy: 0.5428\n",
            "Epoch 2/16\n",
            "63/63 [==============================] - 7s 117ms/step - loss: 1.3808 - accuracy: 0.5904 - val_loss: 1.2605 - val_accuracy: 0.6204\n",
            "Epoch 3/16\n",
            "24/63 [==========>...................] - ETA: 4s - loss: 1.2126 - accuracy: 0.6356"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv-bNUhnxOET"
      },
      "source": [
        "print(logits_to_text(simple_rnn_model2.predict(tmp_x[:1])[0], plaintext_tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kYK8izQxOEU"
      },
      "source": [
        "En general:\n",
        "- el GRU(Gated Recurrent Unit) tiene la menor cantidad de errores (solo 1) pero requirió un mayor tiempo de entrenamiento con 16 épocas para 128 tamaños de lote\n",
        "- SimpleRNN (recurrent neuronal networking) con el mismo entrenamiento tiene muchos más errores\n",
        "- Finalmente, LSTM (Long short-term memory) tiene solo 2 errores y se entrenó con solo 4 épocas y 64 tamaños de lote, que es dos veces menor que el GRU\n",
        "\n",
        "Conclusión: el mejor rendimiento para LSTM con GRU "
      ]
    }
  ]
}